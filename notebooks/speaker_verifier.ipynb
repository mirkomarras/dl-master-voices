{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Speaker Models\n",
    "\n",
    "This notebook allows you to test pre-trained speaker models in terms fo equal error rate and impersonation rate against master voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import roc_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **test_set**: id of the dataset against which equal error rate is computed ('vox1' or 'vox2'); please refer to speaker model testing. \n",
    "- **nets** ['{net1}/{vxxx}', '{net2}/{vxxx}', ..., '{netn}/{vxxx}']: comma-separated list of speaker models to test\n",
    "- **tars** [None, 1.0, 0.1]: comma-separated list of false acceptance levels to test (None stands for EER level; other common values are 1.0 and 0.1)\n",
    "- **pols** ['avg', 'any']: comma-separated list of verification policies to test\n",
    "- **thrs_types** [None, 'avg', 'any']: comma-separated list of thresholds to against which master voice impersonation is tested (None stands for raw 1-1 comparisons)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = 'vox1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = ['vggvox/v003','resnet50/v003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tars = [None, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols = ['avg', 'any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs_types = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of similarity scores and verification labels (0:diff-user pair, 1:same-user pair), this function computes the verification threshold, FAR, and FRR at a given target_fa false acceptance level. If target_fa=None, this function computes threshold, FAR, and FAR at the equal error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneThreshold(scores, labels, target_fa=None):\n",
    "    far, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    frr = 1 - tpr\n",
    "    frr = frr*100\n",
    "    far = far*100\n",
    "    if target_fa:\n",
    "        idx = np.nanargmin(np.absolute((target_fa - far))) \n",
    "        return thresholds[idx], far[idx], frr[idx]\n",
    "    idxE = np.nanargmin(np.absolute((frr - far)))\n",
    "    eer  = max(far[idxE], frr[idxE])\n",
    "    return thresholds[idxE], far[idxE], frr[idxE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define to functions in order to compute false acceptance and false rejection rates at a given verification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_far(labels, scores, thr):\n",
    "    fars = 0\n",
    "    count = 0\n",
    "    for t, s in zip(labels, scores):\n",
    "        if t == 0:\n",
    "            if s >= thr:\n",
    "                fars += 1\n",
    "            count += 1\n",
    "    return fars / count * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frr(labels, scores, thr):\n",
    "    frrs = 0\n",
    "    count = 0\n",
    "    for t, s in zip(labels, scores):\n",
    "        if t == 1:\n",
    "            if s < thr:\n",
    "                frrs += 1\n",
    "            count += 1\n",
    "    return frrs / count * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the csv files computed under the speaker model testing procedure, for all the specified speaker models. Each file includes verification labels and corresponding similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox1_test_results = {}\n",
    "for net in nets:\n",
    "    vox1_test_results[net] = pd.read_csv(os.path.join('../data/vs_mv_models', net, 'scores_' + test_set + '_test.csv'))\n",
    "    vox1_test_results[net] = vox1_test_results[net].loc[:, ~vox1_test_results[net].columns.str.contains('^Unnamed')]\n",
    "    vox1_test_results[net].columns = ['label', 'score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each speaker model, we also retrieve the training history (e.g., loss, accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_results = {}\n",
    "for net in nets:\n",
    "    train_history_results[net] = pd.read_csv(os.path.join('../data/vs_mv_models', net, 'history.csv'))\n",
    "    train_history_results[net] = train_history_results[net].loc[:, ~train_history_results[net].columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each speaker model and each user within the corresponding list of trial pairs, we compute the maximum similarity score for same-user pairs and different-user pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupScores(scores, labels, size=8): # By default, 8 trial pairs per user are included into the vox1_test_results CSVs\n",
    "    if thrs_type is None:\n",
    "        return scores, labels\n",
    "    grp_scores, grp_labels = [], []\n",
    "    for i in range(0, len(scores), size): # For all the trial pairs of a user\n",
    "        curr_scores = scores[i:i+size] # We retrieve the similarity scores for his/her trial pairs \n",
    "        grp_scores.append(np.max(curr_scores[1::2])) # We get the maximum similarity score for different-user trial pairs\n",
    "        grp_labels.append(0)\n",
    "        grp_scores.append(np.max(curr_scores[0::2])) # We get the maximum similarity score for same-user trial pairs\n",
    "        grp_labels.append(1)\n",
    "    return grp_scores, grp_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet creates and populates to dictionaries:\n",
    "- **ress** will include the performance in terms of speaker recognition for each thrs_type, tar, and net (mean far-frr, far, frr, thr, no_trials), loss, acc]);\n",
    "- **thrs** will include the thresholds for each thrs_type, tar, net (threshold value in [-1, 1]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = {} \n",
    "thrs = {}\n",
    "for thrs_type in thrs_types: # For all the threshold types, e.g., [None, 1.0, 0.1]\n",
    "    ress[thrs_type] = {}\n",
    "    thrs[thrs_type] = {}\n",
    "    for tar in tars: # For all the false acceptance rates, e.g., [None, 1.0, 0.1] \n",
    "        ress[thrs_type][tar] = {}\n",
    "        thrs[thrs_type][tar] = {}\n",
    "        for net in nets: # For all the speaker models to test\n",
    "            loss = train_history_results[net]['loss'].values[-1] # We get the training loss for the last epoch\n",
    "            acc = train_history_results[net]['acc'].values[-1] # We get the training accuracy for the last epoch\n",
    "            if thrs_type is None: # If we want to use the threshold at EER\n",
    "                thr, far, frr = tuneThreshold(vox1_test_results[net]['score'].values, vox1_test_results[net]['label'].values, tar)\n",
    "                thrs[thrs_type][tar][net] = thr\n",
    "                ress[thrs_type][tar][net] = [np.mean([far, frr]), far, frr, thr, len(vox1_test_results[net].index), loss, acc]\n",
    "            else: # If we want to use a threshold different from the one at EER\n",
    "                grp_scores, grp_labels = groupScores(vox1_test_results[net]['score'].values, vox1_test_results[net]['label'].values, thrs_type)\n",
    "                thr = thrs[None][tar][net]\n",
    "                far = count_far(grp_labels, grp_scores, thr)\n",
    "                frr = count_frr(grp_labels, grp_scores, thr)\n",
    "                ress[thrs_type][tar][net] = [np.mean([far, frr]), far, frr, thr, len(vox1_test_results[net].index), loss, acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the speaker recognition performance for each speaker model at different thrs_types and false acceptance levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thrs_type in thrs_types:\n",
    "    for tar in tars:\n",
    "        tar_label = (thrs_type if thrs_type is not None else 'raw') + '  ' + ('FAR@'+str(tar) if tar is not None else 'EER')\n",
    "        df = pd.DataFrame.from_dict(ress[thrs_type][tar], orient='index', columns=['eer', 'far', 'frr', 'thr', 'no-trials', 'loss', 'acc'])\n",
    "        df.columns = pd.MultiIndex.from_tuples([(tar_label,'eer'), (tar_label,'far'), (tar_label,'frr'), (tar_label, 'thr'), (tar_label, 'no-trials'), (tar_label, 'loss'), (tar_label, 'acc')])\n",
    "        df.style.set_properties(**{'width':'10em', 'text-align':'center'})\n",
    "        df.sort_index(inplace=True)\n",
    "        display(HTML(df.to_html()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Impersonation Performance\n",
    "\n",
    "The following code will allow you to compute the impersonation rate of all the considered speaker models against master voices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a master voice csv file with similarity scores, paths of compared audio, and user's gender, this function will compute the impersonation rate for male and female users. Specifically, it returns five values:\n",
    "- **imp_m**: number of male users who have been impersonated\n",
    "- **imp_f**: number of female users who have been impersonated\n",
    "- **user_ids_m**: list of male user ids who have been impersonated  \n",
    "- **user_ids_f**: list of female user ids who have been impersonated\n",
    "- **tot_m**: number of male users\n",
    "- **tot_f**: number of female users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeImpersonation(mv_csv_file, thr, size=10):\n",
    "    df = pd.read_csv(mv_csv_file)\n",
    "    imp_m, tot_m = 0, 0\n",
    "    imp_f, tot_f = 0, 0\n",
    "    user_ids_f, user_ids_m = [], []\n",
    "    for i in range(0, len(df), size) : \n",
    "        user_id = i // size\n",
    "        tot_f += 1 if df.loc[i, 'gender'] == 'f' else 0\n",
    "        tot_m += 1 if df.loc[i, 'gender'] == 'm' else 0\n",
    "        imp_r = len([i for i in df.loc[i:i+size-1, 'score'] if i >= thr]) \n",
    "        imp_f += 1 if df.loc[i, 'gender'] == 'f' and imp_r > 0 else 0\n",
    "        imp_m += 1 if df.loc[i, 'gender'] == 'm' and imp_r > 0 else 0\n",
    "        user_ids_f += [user_id] if df.loc[i, 'gender'] == 'f' and imp_r > 0 else []\n",
    "        user_ids_m += [user_id] if df.loc[i, 'gender'] == 'm' and imp_r > 0 else []\n",
    "    assert imp_m / tot_m <= 1.0 and imp_f / tot_f <= 1.0\n",
    "    return imp_m, imp_f, user_ids_m, user_ids_f, tot_m, tot_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet will create a dictionary with he impersonation results for different policies, false acceptance levels, speaker models, master voice sets, and master voice files. Specifically, the resulting mv_test_results will include the imp_m, imp_f, user_ids_m, user_ids_f, tot_m, tot_f for each master voice file. See what the function computeImpersonation returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_no_samples_per_mvset = 10\n",
    "no_templates_per_user = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_test_results = {}\n",
    "for i1, pol in enumerate(pols): # For each verification policy\n",
    "    mv_test_results[pol] = {}\n",
    "    for i2, tar in enumerate(tars): # For each false acceptance level\n",
    "        mv_test_results[pol][tar] = {}\n",
    "        for i3, net in enumerate(nets): # For each speaker model\n",
    "            mv_test_results[pol][tar][net] = {}\n",
    "            dp = os.path.join('../data/vs_mv_models', net, 'mvcmp_' + pol)\n",
    "            for i4, mvset in enumerate(os.listdir(os.path.join(dp))): # For each master voice set\n",
    "                mv_test_results[pol][tar][net][mvset] = {}\n",
    "                for i5, version in enumerate(os.listdir(os.path.join(dp, mvset))): # For each version of a master voice set\n",
    "                    mv_test_results[pol][tar][net][mvset][version] = {}\n",
    "                    for mvsam in os.listdir(os.path.join(dp, mvset, version)): # For each master voice file in the current set\n",
    "                        if int(mvsam.split('.')[0].split('_')[-1]) <= max_no_samples_per_mvset: # We get only max_no_samples_per_mvset at maximum \n",
    "                            # We compute the impersonation results for the current master voice file mvsam (if the policy is 'any', we need to combine no_templates_per_user similarity scores)\n",
    "                            score_group_size = 1 if pol == 'avg' else no_templates_per_user\n",
    "                            mv_test_results[pol][tar][net][mvset][version][mvsam] = computeImpersonation(os.path.join(dp, mvset, version, mvsam), thrs[None][tar][net], score_group_size) \n",
    "                            print('>\\r', pol, '(' + str(i1+1) + '/' + str(len(pols)) + ')', tar, '(' + str(i2+1) + '/' + str(len(tars)) + ')', \n",
    "                                         net, '(' + str(i3+1) + '/' + str(len(nets)) + ')',  mvset, '('+str(i4+1)+'/'+str(len(os.listdir(dp))) +')',  \n",
    "                                         version, '('+str(i5+1)+'/'+str(len(os.listdir(os.path.join(dp, mvset)))) +')', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the percentage of male and female users impersonated by a given set of master voice sets within a specific speaker model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeData(data, ress, no_verification_trials=1):\n",
    "    for mvset, mvversions in data.items(): # For each master voice set \n",
    "        for mvversion, mvsamps in mvversions.items(): # For each version of the current master voice set \n",
    "            imp_m = []\n",
    "            imp_f = []\n",
    "            if no_verification_trials <= 1: # If we have only one verification trial, we consider all the master voice files separately\n",
    "                for mvsam, mvress in mvsamps.items(): # For each master voice file, we compute the percentage of male and female users who have been impersonated\n",
    "                    imp_m.append(mvress[0] / mvress[4]) # Percentage of male users impersonated by the current master voice file mvsam\n",
    "                    imp_f.append(mvress[1] / mvress[5]) # Percentage of female users impersonated by the current master voice file mvsam\n",
    "            else: # If we have more than one verification trial, we consider the master voice files in the current set as a group\n",
    "                keys = list(mvsamps.keys()) # We get and sort all the master voice filenames, first we try with the master voice file at keys[0], then at keys[1], and so on\n",
    "                keys.sort()\n",
    "                tot_m, tot_f = 0, 0\n",
    "                for t in range(min(no_verification_trials, len(keys))): # For each verification trial we have\n",
    "                    imp_m += mvsamps[keys[t]][2] # We add to the list of impersonated male user IDs those IDs that have been impersonated by keys[t] master voice file\n",
    "                    imp_f += mvsamps[keys[t]][3] # We add to the list of impersonated female user IDs those IDs that have been impersonated by keys[t] master voice file\n",
    "                    tot_m = mvsamps[keys[t]][4] # We keep trace of the total number of male users\n",
    "                    tot_f = mvsamps[keys[t]][5] # We keep trace of the total number of female users\n",
    "                imp_m = [len(set(imp_m)) / tot_m] # We compute the percentage of male users who have been impersonated after the no_verification_trials trials \n",
    "                imp_f = [len(set(imp_f)) / tot_f] # We compute the percentage of female users who have been impersonated after the no_verification_trials trials \n",
    "            if mvset + '-' + mvversion not in ress: # If this is the first time we see this master voice set and version, we initialize the row\n",
    "                ress[mvset + '-' + mvversion] = [round(np.mean(imp_m)*100,2), round(np.mean(imp_f)*100,2)]\n",
    "            else: # We append to the wrow of this master voice set and version, the impersonation rates achieved against the current speaker model \n",
    "                ress[mvset + '-' + mvversion] += [round(np.mean(imp_m)*100,2), round(np.mean(imp_f)*100,2)]\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each policy and falce acceptance level, we show the impersonation results of all the master voice sets against all the considered speaker models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_allowed_verification_trials = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pol in pols: # For all the verification policies\n",
    "    for tar in tars: # For all the false acceptance levels\n",
    "        rows = {}\n",
    "        cols = []\n",
    "        for net in nets: # For all the speaker models \n",
    "            rows = arrangeData(mv_test_results[pol][tar][net], rows, no_allowed_verification_trials) # We add imp rates of the current mv set against the current speaker model \n",
    "            cols += [net + '-m', net + '-f'] # We add two new columns associated to the male/female impersonation rates for the current speaker model \n",
    "        tar_label = pol.upper() + '  ' + ('FAR@'+str(tar) if tar is not None else 'EER') # We define the table title \n",
    "        df = pd.DataFrame.from_dict(rows, orient='index', columns=cols) # We create a dataframe starting from the rows and cols we initialized\n",
    "        df = df.mask(df==0).fillna('-') # To improve readability, we replace 0% impersonation rates with '-'\n",
    "        df.sort_index(inplace=True) # We sort the master voice sets alphetically\n",
    "        df.columns = pd.MultiIndex.from_tuples([(tar_label, col.split('-')[0], col.split('-')[1]) for col in cols])\n",
    "        df.style.set_properties(**{'width':'10em', 'text-align':'center'})\n",
    "        display(HTML(df.to_html()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
