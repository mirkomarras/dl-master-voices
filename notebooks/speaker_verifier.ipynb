{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking ASVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import roc_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- nets ['arch/vxxx']: comma-separated list of ASVs to test\n",
    "- tars [None, 1.0, 0.1]: comma-separated list of false acceptance levels to test (None stands for EER level)\n",
    "- pols ['avg', 'any']: comma-separated list of verification policies to test\n",
    "- thrs_types [None, 'avg', 'any']: comma-separated list of thresholds to select across policies (None stands for EER level)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = 'vox1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = ['xvector/v000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tars = [None, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols = ['avg', 'any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs_types = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneThreshold(scores, labels, target_fa=None):\n",
    "    far, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    frr = 1 - tpr\n",
    "    frr = frr*100\n",
    "    far = far*100\n",
    "    if target_fa:\n",
    "        idx = np.nanargmin(np.absolute((target_fa - far))) \n",
    "        return thresholds[idx], far[idx], frr[idx]\n",
    "    idxE = np.nanargmin(np.absolute((frr - far)))\n",
    "    eer  = max(far[idxE], frr[idxE])\n",
    "    return thresholds[idxE], far[idxE], frr[idxE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_far(targets, similarities, thr):\n",
    "    fars = 0\n",
    "    count = 0\n",
    "    for t, s in zip(targets, similarities):\n",
    "        if t == 0:\n",
    "            if s >= thr:\n",
    "                fars += 1\n",
    "            count += 1\n",
    "    return fars / count * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frr(targets, similarities, thr):\n",
    "    frrs = 0\n",
    "    count = 0\n",
    "    for t, s in zip(targets, similarities):\n",
    "        if t == 1:\n",
    "            if s < thr:\n",
    "                frrs += 1\n",
    "            count += 1\n",
    "    return frrs / count * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox1_test_results = {}\n",
    "for net in nets:\n",
    "    vox1_test_results[net] = pd.read_csv(os.path.join('../data/pt_models', net, 'scores_' + test_set + '_test.csv'))\n",
    "    vox1_test_results[net] = vox1_test_results[net].loc[:, ~vox1_test_results[net].columns.str.contains('^Unnamed')]\n",
    "    vox1_test_results[net].columns = ['label', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_results = {}\n",
    "for net in nets:\n",
    "    train_history_results[net] = pd.read_csv(os.path.join('../data/pt_models', net, 'history.csv'))\n",
    "    train_history_results[net] = train_history_results[net].loc[:, ~train_history_results[net].columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupScores(scores, labels, thrs_type, size=8):\n",
    "    if thrs_type is None:\n",
    "        return scores, labels\n",
    "    func = np.mean if thrs_type == 'avg' else np.max\n",
    "    grp_scores, grp_labels = [], []\n",
    "    for i in range(0, len(scores), size):\n",
    "        curr_scores = scores[i:i+size]\n",
    "        grp_scores.append(func(curr_scores[1::2]))\n",
    "        grp_labels.append(0)\n",
    "        grp_scores.append(func(curr_scores[0::2]))\n",
    "        grp_labels.append(1)\n",
    "    return grp_scores, grp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = {}\n",
    "thrs = {}\n",
    "for thrs_type in thrs_types:\n",
    "    ress[thrs_type] = {}\n",
    "    thrs[thrs_type] = {}\n",
    "    for tar in tars:\n",
    "        ress[thrs_type][tar] = {}\n",
    "        thrs[thrs_type][tar] = {}\n",
    "        for net in nets:\n",
    "            loss = train_history_results[net]['loss'].values[-1]\n",
    "            acc = train_history_results[net]['acc'].values[-1]\n",
    "            if thrs_type is None:\n",
    "                thr, far, frr = tuneThreshold(vox1_test_results[net]['score'].values, vox1_test_results[net]['label'].values, tar)\n",
    "                thrs[thrs_type][tar][net] = thr\n",
    "                ress[thrs_type][tar][net] = [np.mean([far, frr]), far, frr, thr, len(vox1_test_results[net].index), loss, acc]\n",
    "            else:\n",
    "                grp_scores, grp_labels = groupScores(vox1_test_results[net]['score'].values, vox1_test_results[net]['label'].values, thrs_type)\n",
    "                thr = thrs[None][tar][net]\n",
    "                far = count_far(grp_labels, grp_scores, thr)\n",
    "                frr = count_frr(grp_labels, grp_scores, thr)\n",
    "                ress[thrs_type][tar][net] = [np.mean([far, frr]), far, frr, thr, len(vox1_test_results[net].index), loss, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thrs_type in thrs_types:\n",
    "    for tar in tars:\n",
    "        tar_label = (thrs_type if thrs_type is not None else 'raw') + '  ' + ('FAR@'+str(tar) if tar is not None else 'EER')\n",
    "        df = pd.DataFrame.from_dict(ress[thrs_type][tar], orient='index', columns=['eer', 'far', 'frr', 'thr', 'no-trials', 'loss', 'acc'])\n",
    "        df.columns = pd.MultiIndex.from_tuples([(tar_label,'eer'), (tar_label,'far'), (tar_label,'frr'), (tar_label, 'thr'), (tar_label, 'no-trials'), (tar_label, 'loss'), (tar_label, 'acc')])\n",
    "        df.style.set_properties(**{'width':'10em', 'text-align':'center'})\n",
    "        df.sort_index(inplace=True)\n",
    "        display(HTML(df.to_html()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeImpersonation(fp, thr, pol, size=10):\n",
    "    df = pd.read_csv(fp)\n",
    "    imp_m, tot_m = 0, 0\n",
    "    imp_f, tot_f = 0, 0\n",
    "    user_ids_f, user_ids_m = [], []\n",
    "    for i in range(0, len(df), size) : \n",
    "        user_id = i // size\n",
    "        tot_f += 1 if df.loc[i, 'gender'] == 'f' else 0\n",
    "        tot_m += 1 if df.loc[i, 'gender'] == 'm' else 0\n",
    "        imp_r = len([i for i in df.loc[i:i+size-1, 'score'] if i >= thr]) if pol == 'any' else (1 if np.mean(df.loc[i:i+size-1, 'score']) > thr else 0)\n",
    "        imp_f += 1 if df.loc[i, 'gender'] == 'f' and imp_r > 0 else 0\n",
    "        imp_m += 1 if df.loc[i, 'gender'] == 'm' and imp_r > 0 else 0\n",
    "        user_ids_f += [user_id] if df.loc[i, 'gender'] == 'f' and imp_r > 0 else []\n",
    "        user_ids_m += [user_id] if df.loc[i, 'gender'] == 'm' and imp_r > 0 else []\n",
    "    assert imp_m / tot_m <= 1.0 and imp_f / tot_f <= 1.0\n",
    "    return imp_m, imp_f, user_ids_m, user_ids_f, tot_m, tot_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_test_results = {}\n",
    "for i1, pol in enumerate(pols):\n",
    "    mv_test_results[pol] = {}\n",
    "    for i2, tar in enumerate(tars):\n",
    "        mv_test_results[pol][tar] = {}\n",
    "        for i3, net in enumerate(nets):\n",
    "            mv_test_results[pol][tar][net] = {}\n",
    "            dp = os.path.join('../data/pt_models', net, 'mvcmp_any')\n",
    "            for i4, mvset in enumerate(os.listdir(os.path.join(dp))): \n",
    "                for mvsam in os.listdir(os.path.join(dp, mvset, 'v000')):\n",
    "                    if int(mvsam.split('.')[0].split('_')[-1]) <=5:\n",
    "                        if mvset not in mv_test_results[pol][tar][net]:\n",
    "                            mv_test_results[pol][tar][net][mvset] = {}\n",
    "                        mv_test_results[pol][tar][net][mvset][mvsam] = computeImpersonation(os.path.join(dp, mvset, 'v000', mvsam), thrs[None][tar][net], pol) \n",
    "                        print('>\\r', pol, '(' + str(i1+1) + '/' + str(len(pols)) + ')', tar, '(' + str(i2+1) + '/' + str(len(tars)) + ')', \n",
    "                                     net, '(' + str(i3+1) + '/' + str(len(nets)) + ')',  mvset, '('+str(i4+1)+'/'+str(len(os.listdir(dp))) +')', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeData(data, ress, no_trials=1):\n",
    "    for mvset, mvsamps in data.items():\n",
    "        imp_m = []\n",
    "        imp_f = []\n",
    "        if no_trials <= 1:\n",
    "            for mvsam, mvress in mvsamps.items():\n",
    "                imp_m.append(mvress[0] / mvress[4])\n",
    "                imp_f.append(mvress[1] / mvress[5])\n",
    "        else:\n",
    "            keys = list(mvsamps.keys())\n",
    "            keys.sort()\n",
    "            tot_m, tot_f = 0, 0\n",
    "            for t in range(min(no_trials, len(keys))):\n",
    "                imp_m += mvsamps[keys[t]][2]\n",
    "                imp_f += mvsamps[keys[t]][3]\n",
    "                tot_m = mvsamps[keys[t]][4]\n",
    "                tot_f = mvsamps[keys[t]][5]\n",
    "            imp_m = [len(set(imp_m)) / tot_m]\n",
    "            imp_f = [len(set(imp_f)) / tot_f]\n",
    "        if mvset not in ress:\n",
    "            ress[mvset] = [round(np.mean(imp_m)*100,2), round(np.mean(imp_f)*100,2)]\n",
    "        else:\n",
    "            ress[mvset] += [round(np.mean(imp_m)*100,2), round(np.mean(imp_f)*100,2)]\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = ['xvector/v000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pol in pols:\n",
    "    for tar in tars:\n",
    "        ress = {}\n",
    "        cols = []\n",
    "        for net in nets:\n",
    "            ress = arrangeData(mv_test_results[pol][tar][net], ress, no_trials=1) \n",
    "            cols += [net + '-m', net + '-f']\n",
    "        tar_label = pol.upper() + '  ' + ('FAR@'+str(tar) if tar is not None else 'EER')\n",
    "        df = pd.DataFrame.from_dict(ress, orient='index', columns=cols)\n",
    "        df = df.mask(df==0).fillna('-')\n",
    "        df.sort_index(inplace=True)\n",
    "        df.columns = pd.MultiIndex.from_tuples([(tar_label, col.split('-')[0], col.split('-')[1]) for col in cols])\n",
    "        df.style.set_properties(**{'width':'10em', 'text-align':'center'})\n",
    "        display(HTML(df.to_html()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
