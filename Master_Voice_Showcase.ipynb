{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Optimization for Dictionary Attacks on Speaker Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import decimal\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input, Dot\n",
    "import librosa\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from scipy import spatial\n",
    "from scipy.signal import lfilter\n",
    "import soundfile as sf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj,name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Acoustic Features Extraction: Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the parameters for spectrogram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_params = {'max_sec':10, 'bucket_step':1, 'frame_step':0.01, 'sample_rate':16000, 'preemphasis_alpha':0.97, 'frame_len':0.025, 'num_fft':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_spectrum(filename, params):\n",
    "    signal = load_wav(filename, params['sample_rate'])\n",
    "    frames = framesig(signal, frame_len=params['frame_len'] * params['sample_rate'], frame_step=params['frame_step']*params['sample_rate'], winfunc=np.hamming)\n",
    "    fft = abs(np.fft.fft(frames,n=params['num_fft']))\n",
    "    fft_norm, fft_means, fft_stds = normalize_frames(fft.T)\n",
    "    return fft_norm, fft_means, fft_stds\n",
    "\n",
    "def load_wav(filename, sample_rate):\n",
    "    audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    audio = audio.flatten()\n",
    "    return audio\n",
    "\n",
    "def framesig(sig, frame_len, frame_step, winfunc=lambda x: np.ones((x,)), stride_trick=True):\n",
    "    slen = len(sig)\n",
    "    frame_len = int(round_half_up(frame_len))\n",
    "    frame_step = int(round_half_up(frame_step))\n",
    "    if slen <= frame_len:\n",
    "        numframes = 1\n",
    "    else:\n",
    "        numframes = 1 + int(math.ceil((1.0 * slen - frame_len) / frame_step)) # LV\n",
    "\n",
    "    padlen = int((numframes - 1) * frame_step + frame_len)\n",
    "\n",
    "    zeros = np.zeros((padlen - slen,))\n",
    "    padsignal = np.concatenate((sig, zeros))\n",
    "    if stride_trick:\n",
    "        win = winfunc(frame_len)\n",
    "        frames = rolling_window(padsignal, window=frame_len, step=frame_step)\n",
    "    else:\n",
    "        indices = np.tile(np.arange(0, frame_len), (numframes, 1)) + np.tile(np.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n",
    "        indices = np.array(indices, dtype=np.int32)\n",
    "        frames = padsignal[indices]\n",
    "        win = np.tile(winfunc(frame_len), (numframes, 1))\n",
    "\n",
    "    return frames * win\n",
    "\n",
    "def deframesig(frames, siglen, frame_len, frame_step, winfunc=lambda x: np.ones((x,))):\n",
    "    frame_len = round_half_up(frame_len)\n",
    "    frame_step = round_half_up(frame_step)\n",
    "    numframes = np.shape(frames)[0]\n",
    "    assert np.shape(frames)[1] == frame_len, '\"frames\" matrix is wrong size, 2nd dim is not equal to frame_len'\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_len), (numframes, 1)) + np.tile(\n",
    "        np.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n",
    "    indices = np.array(indices, dtype=np.int32)\n",
    "    padlen = (numframes - 1) * frame_step + frame_len\n",
    "\n",
    "    if siglen <= 0: siglen = padlen\n",
    "\n",
    "    rec_signal = np.zeros((padlen,))\n",
    "    window_correction = np.zeros((padlen,))\n",
    "    win = winfunc(frame_len)\n",
    "\n",
    "    for i in range(0, numframes):\n",
    "        window_correction[indices[i, :]] = window_correction[\n",
    "                                               indices[i, :]] + win + 1e-15  # add a little bit so it is never zero\n",
    "        rec_signal[indices[i, :]] = rec_signal[indices[i, :]] + frames[i, :]\n",
    "\n",
    "    rec_signal = rec_signal / window_correction\n",
    "    return rec_signal[0:siglen]\n",
    "\n",
    "    return frames * win\n",
    "\n",
    "def normalize_frames(m,epsilon=1e-12):\n",
    "    frames = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    for v in m:\n",
    "        means.append(np.mean(v))\n",
    "        stds.append(np.std(v))\n",
    "        frames.append((v - np.mean(v)) / max(np.std(v), epsilon))\n",
    "    return np.array(frames), np.array(means), np.array(stds)\n",
    "\n",
    "def denormalize_frames(m, means, stds, epsilon=1e-12):\n",
    "    return np.array([z * max(stds[i],epsilon) + means[i] for i, z in enumerate(m)])\n",
    "\n",
    "def rolling_window(a, window, step=1):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::step]\n",
    "\n",
    "def round_half_up(number):\n",
    "    return int(decimal.Decimal(number).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Bottleneck Features Extraction: VGGVox-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = load_model('vggvox.h5') # VGGVox Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/voxceleb2/dev/id02331/6dSawXx_NNk/00033.m4a',\n",
       " '../data/voxceleb2/dev/id02331/LJdtecWIS3A/00066.m4a']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_paths = load_obj('../data/backup/train_vox2_paths_1000_users') # Pickle File: An array including a list of paths that are used for training: 50 utterance per person \n",
    "utterance_paths[:2] # Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_labels = load_obj('../data/backup/train_vox2_labels_1000_users') # Pickle File: An array including a list of labels that corresponds to paths in utterance_paths\n",
    "utterance_labels[:2] # Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.78697291e-03,  4.98584891e-03,  1.73048340e-02, ...,\n",
       "         3.47861648e-02, -5.03421434e-05, -3.13258581e-02],\n",
       "       [ 8.77610594e-03,  2.80432142e-02,  1.07790232e-02, ...,\n",
       "         4.44943644e-02,  1.52959935e-02, -9.63459723e-03]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_bottleneck_features = np.load('../data/backup/train_vox2_embs_1000_users.npy') # Numpy File: A 2D matrix including the embedding vectors for paths in utterance_paths\n",
    "utterance_bottleneck_features[:2] # Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the *user_id_position* index, e.g., path/to/voxceleb2/dev/user_id/video_id/file.wav -> user_id_position = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path 50000 / 50000"
     ]
    }
   ],
   "source": [
    "user_id_position = 4\n",
    "indexes_male_utterances = []\n",
    "indexes_female_utterances = []\n",
    "vox_metadata = pd.read_csv('vox2_meta.csv', header=None, names=['vid', 'vggid', 'gender', 'set']) # CSV File: A CSV containing the metadata of the VoxCeleb2 dataset\n",
    "for p_index, path in enumerate(utterance_paths):\n",
    "    print('\\rPath', p_index+1, '/', len(utterance_paths), end='')\n",
    "    if vox_metadata.loc[vox_metadata.vid == path.split('/')[user_id_position], 'gender'].values[0] == 'm':\n",
    "        indexes_male_utterances.append(p_index)\n",
    "    else:\n",
    "        indexes_female_utterances.append(p_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Adversarial Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the parameters for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iterations = 100\n",
    "learning_rate = 10\n",
    "min_similarity = -0.25\n",
    "max_similarity = 0.25\n",
    "min_change = 1e-5\n",
    "indexes_optimization = indexes_male_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_extractor = Model(bottleneck.inputs, Flatten()(bottleneck.output))\n",
    "\n",
    "in_a = Input(shape=(512, None, 1))\n",
    "in_b = Input(shape=(512, None, 1))\n",
    "inputs = [in_a, in_b]\n",
    "\n",
    "emb_a = bottleneck_extractor(in_a)\n",
    "emb_b = bottleneck_extractor(in_b)\n",
    "similarity = Dot(axes=1, normalize=True)([emb_a, emb_b])\n",
    "\n",
    "siamese = Model(inputs, similarity)\n",
    "\n",
    "model_input_layer = [siamese.layers[0].input, siamese.layers[1].input]\n",
    "model_output_layer =  siamese.layers[-1].output\n",
    "\n",
    "cost_function = model_output_layer[0][0]\n",
    "\n",
    "gradient_function = K.gradients(cost_function, model_input_layer)[0]\n",
    "\n",
    "grab_cost_and_gradients_from_model = K.function(model_input_layer, [cost_function, gradient_function])\n",
    "\n",
    "filter_gradients = lambda c, g, t1, t2: [g[i] for i in range(len(c)) if c[i] >= t1 and c[i] <= t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fac(spectrogram, bottleneck_extractor, utterance_paths, utterance_bottleneck_features, threshold=0.53):\n",
    "    bottleneck_features = bottleneck_extractor.predict(spectrogram.reshape(1, *spectrogram.shape, 1))[0]\n",
    "    similarities = [1 - spatial.distance.cosine(bottleneck_features, utterance_bottleneck_features[i]) for i in range(len(utterance_paths))]\n",
    "    fac = np.sum([1 for s in similarities if s > threshold])\n",
    "    return fac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the path of the starting waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/100 - FAC 10754"
     ]
    }
   ],
   "source": [
    "starting_path = 'whatever_voice.wav'\n",
    "starting_spectrogram, starting_waveform_mean, starting_waveform_std = get_fft_spectrum(starting_path, acoustic_params)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    costs = []\n",
    "    gradients = []\n",
    "    for index in random.sample(indexes_optimization, batch_size):\n",
    "        base_spectrogram, _, _ = get_fft_spectrum(utterance_paths[index], acoustic_params)\n",
    "        input_pair =  ([np.array([starting_spectrogram.reshape(*starting_spectrogram.shape, 1)]), np.array([base_spectrogram.reshape(*base_spectrogram.shape, 1)])])\n",
    "        cost, gradient = grab_cost_and_gradients_from_model(input_pair)\n",
    "        costs.append(np.squeeze(cost))\n",
    "        gradients.append(np.squeeze(gradient))\n",
    "        \n",
    "    filtered_gradients = filter_gradients(costs, gradients, min_similarity, max_similarity)\n",
    "    \n",
    "    if len(filtered_gradients) > 0:\n",
    "        perturbation = np.mean(filtered_gradients, axis=0) * learning_rate\n",
    "        perturbation = np.clip(perturbation, min_change, None)\n",
    "        starting_spectrogram += perturbation\n",
    "        fac = evaluate_fac(starting_spectrogram, bottleneck_extractor, utterance_paths, utterance_bottleneck_features, threshold=0.53)\n",
    "        print('\\rStep ' + str(iteration + 1) + '/' + str(n_iterations), '- FAC', fac, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Invertion to Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_for_reconstruction(x, params):\n",
    "    frames = framesig(x, frame_len=params['frame_len'] * params['sample_rate'], frame_step=params['frame_step'] * params['sample_rate'], winfunc=np.hamming)\n",
    "    fft_norm = np.fft.fft(frames, n=params['num_fft'])\n",
    "    return fft_norm\n",
    "\n",
    "def istft_for_reconstruction(X, total_lenght,  params):\n",
    "    frames = np.fft.ifft(X, n=params['num_fft'])\n",
    "    x = deframesig(frames, total_lenght, frame_len=params['num_fft'], frame_step=params['frame_step']*params['sample_rate'], winfunc=np.hamming)\n",
    "    return x\n",
    "\n",
    "def reconstruct_signal_griffin_lim(total_lenght, fft, params, iterations):\n",
    "    x_reconstruct = np.random.randn(total_lenght)\n",
    "    n = iterations\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        reconstruction_spectrogram = stft_for_reconstruction(x_reconstruct, params)\n",
    "        reconstruction_angle = np.angle(reconstruction_spectrogram)\n",
    "        proposal_spectrogram = fft * np.exp(1.0j * reconstruction_angle)\n",
    "        prev_x = x_reconstruct\n",
    "        x_reconstruct = istft_for_reconstruction(proposal_spectrogram, total_lenght, params)\n",
    "        diff = np.sqrt(np.sum((fft - abs(proposal_spectrogram))**2)/fft.size)\n",
    "        print('\\rReconstruction iteration: {}/{} RMSE: {} '.format(iterations - n, iterations, diff), end='')\n",
    "    return x_reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the path for the resulting master voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction iteration: 300/300 RMSE: 1.4660382896075883e-17 "
     ]
    }
   ],
   "source": [
    "master_voice_path = 'master_voice.wav'\n",
    "iterations_griffin_lim = 300\n",
    "starting_waveform, starting_rate = librosa.load(starting_path, sr=acoustic_params['sample_rate'], mono=True)\n",
    "denormalized_starting_spectrogram = denormalize_frames(starting_spectrogram, starting_waveform_mean, starting_waveform_std)\n",
    "x_reconstruct = reconstruct_signal_griffin_lim(len(starting_waveform), denormalized_starting_spectrogram.T, acoustic_params, iterations=iterations_griffin_lim)\n",
    "sf.write(master_voice_path, x_reconstruct, acoustic_params['sample_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Voice FAC 2462"
     ]
    }
   ],
   "source": [
    "starting_voice_spectrogram, starting_voice_mean, starting_voice_std = get_fft_spectrum(starting_path, acoustic_params)\n",
    "fac = evaluate_fac(starting_voice_spectrogram, bottleneck_extractor, utterance_paths, utterance_bottleneck_features, threshold=0.53)\n",
    "print('Starting Voice FAC', fac, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Voice FAC 6279"
     ]
    }
   ],
   "source": [
    "master_voice_spectrogram, master_voice_mean, master_voice_std = get_fft_spectrum(master_voice_path, acoustic_params)\n",
    "fac = evaluate_fac(master_voice_spectrogram, bottleneck_extractor, utterance_paths, utterance_bottleneck_features, threshold=0.53)\n",
    "print('Master Voice FAC', fac, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
